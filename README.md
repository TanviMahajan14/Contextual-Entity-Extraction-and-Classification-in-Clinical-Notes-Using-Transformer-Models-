ðŸ§  Contextual Entity Extraction and Classification in Clinical Notes Using Bio_ClinicalBERT

This repository presents the code and findings of my Master's capstone project at Rutgers University, focused on Named Entity Recognition (NER) in clinical notes using advanced transformer-based models, specifically Bio_ClinicalBERT.

ðŸ“Œ Project Overview
Electronic Health Records (EHRs) contain valuable but unstructured clinical data. This project tackles the problem of contextual entity extraction and classification â€” identifying entities like diseases, medications, procedures, etc. â€” from clinical narratives using state-of-the-art Natural Language Processing (NLP) techniques.

ðŸ” Problem Statement
Goal: Extract and classify medical entities from free-text clinical notes using a deep learning approach.

Challenges:

Domain-specific medical language

Context-dependent relationships between terms

Imbalanced data distribution across entity types

ðŸ’¡ Models Explored
Model	Type	F1-Score	Accuracy
Logistic Regression	Traditional ML	0.67	0.75
Naive Bayes	Traditional ML	0.67	0.75
CRF	Sequence Labeling	0.76	0.78
Bio_ClinicalBERT	Transformer (DL)	0.83	0.91

âœ… Bio_ClinicalBERT significantly outperformed traditional models in precision, recall, and F1-score.

ðŸ§¬ Dataset
MACCROBAT 2018â€“2020 (from Hugging Face)

42 labeled clinical entity types (disease, procedure, medication, etc.)

7,350 sentences with 118,000+ annotations

Preprocessed with token alignment, abbreviation normalization, and BIO tagging

âš™ï¸ Technical Highlights
Fine-tuned Bio_ClinicalBERT on token-level classification with 82 BIO labels

Employed Hugging Face Transformers and AutoTokenizer

Hyperparameter tuning: learning rate (2e-5), batch size (8), 20 epochs

Used dropout, weight decay, and cosine learning rate decay for regularization

ðŸ“Š Evaluation Metrics
Overall F1-Score: 0.83

Top-performing categories: Age, Sex, Medication (F1 > 0.90)

Entity-specific insights: Imbalanced/rare entities (e.g., Volume, Device) underperformed

ðŸ§  Key Learnings
Transformer models, when fine-tuned on domain-specific data, drastically improve entity extraction in clinical NLP tasks.

Careful handling of class imbalance, memory constraints, and hyperparameters is essential for high-impact NLP in healthcare.

ðŸš€ Future Directions
Incorporating larger and more diverse clinical datasets

Experimenting with multi-task learning or ensemble models

Integration into real-time clinical decision-support systems
